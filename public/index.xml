<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Eugene</title>
        <link>http://localhost:1313/</link>
        <description>Recent content on Eugene</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 21 Apr 2025 19:57:54 +0800</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>考研经验分享贴</title>
        <link>http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/</link>
        <pubDate>Mon, 21 Apr 2025 19:57:54 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/</guid>
        <description>&lt;h1 id=&#34;考研经验分享&#34;&gt;考研经验分享：
&lt;/h1&gt;
&lt;h2 id=&#34;个人考后感受真实&#34;&gt;个人考后感受（真实）：
&lt;/h2&gt;
&lt;p&gt;先说一下我的整个的流程：我大概是4月份左右开的，一开始一直想着多手抓，其实最后还是把大量的精力放在学习量子力学上，后来到5-6月份，准备期末考试（热力学与统计物理，固体物理，量子力学真恐怖）以及报南方科技大学夏令营（这里后面会有提及），从夏令营回来之后，我一腔热血的开始准备考研了，但是准备了1个月左右买，就觉得学校吃的不行，睡觉也睡的不太好，所以在8月份我就回家复习备考；在家复习备考的过程就是普物和量子两手抓。后来在8月中旬我又去参加了上海大学的夏令营，在那里认识了我现在这个导师胡晓老师。直到9月份，我回学校之后才算是真正收心准备考研（目标是上海大学的物理学），这时还有100多天，最后初试有惊无险的通过了，复试也算是正常发挥，没有辜负这么多人的期待和我不到4月的努力吧。这就是我整个考研的过程，看起来还是挺丰富的吧，其实总结起来，考研并不难，难在你怎么做出适合你的选择？&lt;/p&gt;
&lt;h2 id=&#34;择校选方向&#34;&gt;择校(选方向)
&lt;/h2&gt;
&lt;p&gt;这里我觉得这个是整个考研过程中最重要的,正所谓“选择大于努力”,所以如果能够提前做出正确的选择,基本上就已经成功了一半.那么怎么样才算成功呢?或者说适合自己呢?&lt;/p&gt;
&lt;p&gt;首先,如果你的底子不好,并且没有啥比赛经验,我建议你选择一些常年招生人数多的学校(211,双非,末流985).因为这样你可以有足够的时间打基础,看自己喜欢研究什么方向,给自己留试错的空间,基础也是你之后做研究的根本,你可以选择考985,当然这是具有挑战的,但是也还是有许多先例的,但是有没有必要呢?(这个问题问一问你自己,如果觉得有必要,那就没必要往下看了,考985,如果你是抱着说我考上研究生就会舒服,我能告诉你,社会阶级只会越来越卷,找到适合自己的路比这么一个虚无缥缈的title更有价值).&lt;/p&gt;
&lt;p&gt;然后,底子还算好,但是没有啥比赛经验的,我建议你问一下其他学长(笑).&lt;/p&gt;
&lt;p&gt;最后,有比赛经验的同学,你们可以往上冲985,因为我相信你们有这份勇气和毅力(并不是说前面的人没有这些,只不过他们把握住了这样的机会).但是还请你们斟酌再三,你们都是有勇气,有毅力,我相信你们能够很好坚持考研的这些事,方向对你们来说也很重要.如果你上了一个985,最后只能做你不喜欢的方向,这并不是一个好的结果.所以提前了解你想考的学校,然后根据你的本科成绩排名以及比赛获奖的情况去报对应学校的夏令营(具体可以找我,但是其实就是海投,不要怕麻烦,或者说怕自己是双非怎么怎么样,又或者没有保研资格).&lt;/p&gt;
&lt;h2 id=&#34;英语&#34;&gt;英语:
&lt;/h2&gt;
&lt;p&gt;我考研英语:50分.我发挥的不太好,但是这是有原因的,我并没有认真对待它&amp;mdash;单词没有认真背,真题没有认真复盘.但是,经过了这么久英语的学习,我还是觉得&lt;strong&gt;看美剧&lt;/strong&gt;真的有效果(如果有需要可以找我下资源).通过看美剧,我在复试的时候和一个老师用英文单聊了3min左右.整个英语应该是&lt;strong&gt;每天短时间和长战线&lt;/strong&gt;(持续很久).&lt;/p&gt;
&lt;h2 id=&#34;量子力学&#34;&gt;量子力学:
&lt;/h2&gt;
&lt;p&gt;我的量子力学是:127分.量子力学可以说我是学了3遍吧,第一遍是跟着&lt;strong&gt;陈童老师的课&lt;/strong&gt;学,第二遍是自学Griffith(跟着陈童老师同步学习),第三遍是MIT的Quantum Mechanic(&lt;a class=&#34;link&#34; href=&#34;https://ocw.mit.edu/courses/8-05-quantum-physics-ii-fall-2013/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://ocw.mit.edu/courses/8-05-quantum-physics-ii-fall-2013/&lt;/a&gt;)这个对于英语的需求很高,但是是一个很好的资源.我可以说量子力学是最需要时间的,也是最不需要时间的.前者是因为,要学好学懂很难,后者是因为如果你单纯是以考试为目的,就直接Justchan7和小黄书那一套也可以达成目的.&lt;/p&gt;
&lt;p&gt;至于&lt;strong&gt;做题&lt;/strong&gt;:我的小黄书没刷完(就前三章刷完了),Griffith的题我十分建议你去钻研,连续性很好(后面的题是前面的延伸).&lt;/p&gt;
&lt;p&gt;然后书籍的推荐:Griffith的量子力学导论但不局限于一本,什么书都可以去参考看看,但是要有目的性(要学什么东西).最后真题的话,最后一个月刷就可以了,到最后你做真题遇不到你做不出的题就差不多了.&lt;/p&gt;
&lt;h2 id=&#34;夏令营问题具体可以找我私聊最后有联系方式&#34;&gt;夏令营问题(具体可以找我私聊,最后有联系方式):
&lt;/h2&gt;
&lt;p&gt;对于夏令营,我应该算是参加的最多的那一个(但是不是最成功的那一个),所以我知道这个适合什么样的人群去参加:有比赛经验,且成绩排名还不错的同学,但是你报一下,万一进去了呢.我知道有很多人怕麻烦,所以我会和你说报夏令营的好处是什么?&lt;/p&gt;
&lt;p&gt;好处是:如果你进了夏令营,就算你没有保研资格,你到后面复试你也会更有优势,这里的优势指的是:你可以在复试的时候以一种更加自如的心态去面对老师,并且你可以提前了解老师的研究方向(我觉得没有一个老师会拒绝一个主动的学生,况且这个学生甚至在做一件可能没有什么回报的事).
说完这些&lt;strong&gt;直接的好处&lt;/strong&gt;,说一下&lt;strong&gt;间接&lt;/strong&gt;的:参加夏令营可以提高你的胆识,我一直觉得我们这个层级的学生和那些985,211的学生之间的差距(天赋,勇气),天赋我觉得无关紧要,更重要的是我们缺少这份勇气,每一个考研人都觉得考研就已经迈出了很大的一步,但是根据我所见过的名校的学生(985,剑桥),它们在本科期间就已经做了很多很多的事情,这里并不是说贬低我们自己,而是实事求是,我们每个人都在走我们自己的路,但是不妨出去看看,其他人是什么样子的,并且大方的表达自己,人生还很长,未来是不一定,没必要去争一个谁输谁赢,自己满意就好了.所以最后给自己一个成长的机会,无论结果如何,去试试.&lt;/p&gt;
&lt;p&gt;制作不易,希望有所帮助!&lt;/p&gt;
&lt;h2 id=&#34;书店&#34;&gt;书店:
&lt;/h2&gt;
&lt;p&gt;下面的书籍都是我考研过程中买的书,如果学弟学妹有需要的可以联系我(有一定收费,但一定更便宜很多):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/1.jpg&#34;
	width=&#34;3024&#34;
	height=&#34;4032&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/1_hu_bfb0fea9901a0af6.jpg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/1_hu_3f4a1bb41273df8c.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;75&#34;
		data-flex-basis=&#34;180px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/2.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/2_hu_6da003cdb3d72f7.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/2_hu_8b8ba7334d3b8bc0.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/3.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/3_hu_556c86e7b65866b8.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/3_hu_e9228093802d54e2.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/4.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/4_hu_5b5735cc3c4fb9ce.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/4_hu_6ffd5f0701274e09.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/5.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/5_hu_efcd032ebe92690e.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/5_hu_b15993a4a76271d6.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/6.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/6_hu_3cde0691ee3ebc7a.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/6_hu_a7455589c2aba27c.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;6&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/7.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/7_hu_1d28518ce235fc32.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/7_hu_2b0c50514e44812b.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;7&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/8.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/8_hu_3d86b397007ca501.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/8_hu_6fcce7d60d2f08fa.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;8&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/9.jpeg&#34;
	width=&#34;4032&#34;
	height=&#34;3024&#34;
	srcset=&#34;http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/9_hu_952ed093ea9f6b4e.jpeg 480w, http://localhost:1313/2025/%E8%80%83%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E8%B4%B4/9_hu_bc0ddd0715593541.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;9&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;联系方式&#34;&gt;联系方式:
&lt;/h2&gt;
&lt;h3 id=&#34;qq1796784184&#34;&gt;QQ:1796784184
&lt;/h3&gt;
&lt;h3 id=&#34;wechat15970835051phone-number&#34;&gt;Wechat:15970835051(Phone Number)
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;希望每一个考研的同学都可以顺利上岸,希望这篇文章可以帮助到你!&lt;/strong&gt;(如果有考&lt;strong&gt;上海大学&lt;/strong&gt;的可以直接联系我,我给你真题)&lt;/p&gt;
&lt;h3 id=&#34;版权信息&#34;&gt;版权信息
&lt;/h3&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI|Why Machines Learn?</title>
        <link>http://localhost:1313/2025/aiwhy-machines-learn/</link>
        <pubDate>Sat, 19 Apr 2025 09:34:28 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/aiwhy-machines-learn/</guid>
        <description>&lt;h1 id=&#34;why-machines-learn&#34;&gt;Why machines learn？
&lt;/h1&gt;
&lt;h2 id=&#34;deseperatly-seeking-pattern&#34;&gt;Deseperatly seeking pattern
&lt;/h2&gt;
&lt;h3 id=&#34;从现象出发&#34;&gt;从现象出发
&lt;/h3&gt;
&lt;p&gt;这里面的pattern其实就是数据中某种模式特征，打个比方，每一个人对于每一年每一个季度的天气的理解和判断，都是基于一段时间（几年）的生活，通过观察每一年的天气（数据），然后得出一些经验（特征）。&lt;/p&gt;
&lt;p&gt;然而令人惊讶的是，小小的鸭苗在没有父母的帮助下，也可以从运动的物体中找到一定的规律，这些规律可以是相似之处，也可以是不同之处，比如，小鸭苗如果看见有5个黑鸭苗和2个白鸭苗的队伍，它会知道自己是黑的还是白的，并且知道黑色和白色的差别，最后他会加入到其中一个队伍里面。&lt;/p&gt;
&lt;p&gt;这就是令人惊讶的动物的学习能力。在早些年，就有科学家提出“动物（人）是怎么学习的”这一问题，他们就想到先从学习数据中的特征入手，于是就有科学家开发了Perceptron（感知机）来模拟人类的思考。当你给这个感知机下面这些数据的时候：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;$x_1$&lt;/th&gt;
          &lt;th&gt;$x_2$&lt;/th&gt;
          &lt;th&gt;$y$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;4&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;8&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;4&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们只需要通过一点点的观察和心算就能够发现这组数据中存在的关系是：
$$
y = x_1+2x_2
$$&lt;/p&gt;
&lt;p&gt;而对于现在的机器而言无非就是一个Regression算法，这个算法的意思是：你给他很多的train data（有input和output），然后它会通过学习这些数据来给出这些量之间的线性关系（$y =w_1x_1+w_2x_2+b$），也就是学习得到 $w_1,w_2$(系数，weight)以及偏差（截距）。&lt;/p&gt;
&lt;p&gt;然后你可以通过一些测试数据来判读这一组系数的好坏（离最优的系数差多远，最优的系数当然是通过每一个数据点，但是实际上并不能很好的做到），最后得到这组数据的最优解，接下来你就可以通过这样的关系来predict不同的input，会有怎么样的output了。就好像天气一样，你可以通过之前每一天的数据来预测之后的数据一样，只不过天气与很多因素有关。&lt;/p&gt;
&lt;p&gt;所以Regression Method的具体步骤是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;给他一定的训练数据，并且指定一开始的 $w_1,w2，b$&lt;/li&gt;
&lt;li&gt;然后计算这一组的系数所给出的output $y_{predict}和y_{train}$之间的差距，然后通过这个差距反过来调节系数&lt;/li&gt;
&lt;li&gt;不断的执行，直到这个差距小到我们的要求为止&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;神经元的逻辑化&#34;&gt;神经元的逻辑化
&lt;/h3&gt;
&lt;p&gt;言归正传（Regression后面还会系统的说明），我们也许可以通过理解机器是怎么学习的来完全理解人类是怎么学习的（这里我的理解是我们可以在让机器逐步学会学习的过程中，不断地深化我们对于自己学习过程的理解）。&lt;/p&gt;
&lt;p&gt;19世纪，图灵等科学家就认为logic和computation之间有很深厚的联系，他们断言“所有的计算都可以被简化为某种逻辑”。然后就引出了这样一个问题：既然人脑是可以执行计算的，那么它是怎么样执行逻辑操作的（它底层是否像逻辑门一样呢？）。&lt;/p&gt;
&lt;p&gt;带着这样的问题，有生物学家通过类比一个神经元：&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/1.png&#34;
	width=&#34;948&#34;
	height=&#34;421&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/1_hu_4cf4f3e353866372.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/1_hu_f54ef82382b8a7c7.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;截屏2025-04-19 09.05.51&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;540px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;图中Dendrites就是神经元的树突，它负责接受各种刺激，树突中间的就是细胞体，他可以处理树突接受到的刺激（相当于进行计算），然后Axon（轴突）负责转递细胞体的结果到Axon terminals（端粒），端粒在将这个结果传递给周边其他的神经元。&lt;/p&gt;
&lt;p&gt;然后生物学家希望把这一机构转化为一个简单的计算模型，理由是：他很像一个机器，你给他一个输入（刺激），他就会给输出。因此他们想要通过类比用神经元来构建逻辑AND，OR操作。&lt;/p&gt;
&lt;p&gt;他们首先将这个神经元定义为这个样子：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/2025/aiwhy-machines-learn/2.png&#34;
	width=&#34;974&#34;
	height=&#34;382&#34;
	srcset=&#34;http://localhost:1313/2025/aiwhy-machines-learn/2_hu_5ca8c49f76992445.png 480w, http://localhost:1313/2025/aiwhy-machines-learn/2_hu_64a170845a1ba395.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上图的左侧就是给神经元的输入，然后中间的g，f就代表的是神经元对于输入的处理，然后再到右侧的输入y（其实我们可以在g，f的中间再加一个传输的过程，就是将g的处理结果传输到下一个f神经元处）。&lt;/p&gt;
&lt;p&gt;然后这里假设 $x_1，x_2 \in {0,1}$,并且神经元会这样处理输入：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sum  = x1+x2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If  $Sum\geq \theta :y=1$&lt;/p&gt;
&lt;p&gt;else:y = 0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以这里我们就可以认为g其实就是对输入做了一个加法，而f函数就是对g的输出做了一个判断，但是这个 $\theta$要根据具体的情况而定的（这也是人脑的神秘之处），这一整个可以表示为：&lt;/p&gt;
&lt;p&gt;$$
f(g(x)) =
\begin{cases}0, &amp;amp; g(x) &amp;lt; \theta \\
1, &amp;amp; g(x) \geq \theta
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;有了这样的前提，我们就可以来设计基础的布尔逻辑门的操作了。&lt;/p&gt;
&lt;p&gt;首先对于AND逻辑来说：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;x1&lt;/th&gt;
          &lt;th&gt;x2&lt;/th&gt;
          &lt;th&gt;sum&lt;/th&gt;
          &lt;th&gt;x1 And x2&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;我们可以看到这么一个逻辑对于Sum小于等于1的输出都是0，而大于1的就是1，所以我们要神经元完成这个逻辑只需要将 $\theta = 2$ ,得到：&lt;/p&gt;
&lt;p&gt;$$
f(g(x)) =
\begin{cases}0, &amp;amp; g(x) &amp;lt; 2 \\
1, &amp;amp; g(x) \geq 2
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;对于OR逻辑操作也是同理，读者可以自行一试（答案是 $\theta=1$）。&lt;/p&gt;
&lt;p&gt;但是这里有一个比较有意思的问题：但神经元需要处理不同种类的逻辑的时候，他是如何调整这个 $\theta$的值呢？&lt;/p&gt;
&lt;p&gt;Tips:下次记得分段公式要三条斜杆（调试了一上午）&lt;/p&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>毕业论文|chapter 2</title>
        <link>http://localhost:1313/2025/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87chapter-2/</link>
        <pubDate>Wed, 16 Apr 2025 16:39:44 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87chapter-2/</guid>
        <description>&lt;h1 id=&#34;量子纠缠的验证&#34;&gt;量子纠缠的验证
&lt;/h1&gt;
&lt;h2 id=&#34;bell不等式&#34;&gt;Bell不等式
&lt;/h2&gt;
&lt;p&gt;要验证量子纠缠态的存在性，我们首先要证明隐变量理论的矛盾性，这样就证明了非局域性是量子力学的本质特征。前面EPR论文提出了量子力学的不完备性，而应该由额外的变量来补充的论据。这些变量试图去恢复理论中的局域性（&lt;strong&gt;一个系统上的测量结果不受过去与之相互作用且遥远系统的影响&lt;/strong&gt;）和因果性。在之后的一段时间里，有许多试图去完善隐变量理论的工作，但是均已失败告终。&lt;/p&gt;
&lt;p&gt;Bell通过将隐变量理论数学化并加入局域性假设，证明了其与量子力学的统计预测不相容（具有矛盾），即非局域性是量子力学的典型特征。&lt;/p&gt;
&lt;h3 id=&#34;斯特恩-格拉赫实验&#34;&gt;斯特恩-格拉赫实验
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;（斯特恩-格拉赫实验可以展开说）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;考虑一对自旋为$\frac{1}{2}$的粒子，整个系统处于自旋单态（总自旋为0），两个粒子自由地朝反方向运动。自旋单态的表达式为：&lt;/p&gt;
&lt;p&gt;$$
\ket{\psi} = \frac{1}{\sqrt{2}}(\ket{\uparrow \downarrow} - \ket{\downarrow \uparrow})
$$&lt;/p&gt;
&lt;p&gt;式中$\ket{\uparrow\downarrow}$表示两粒子系统的状态，也可写成$\ket{\uparrow} \otimes \ket{\downarrow}$（左边为粒子1的自旋状态），$\ket{\uparrow}$表示单个粒子自旋向上，$\ket{\downarrow}$表示自旋向下。&lt;/p&gt;
&lt;p&gt;定义粒子1和2的自旋算符$\vec{\sigma_1}$和$\vec{\sigma_2}$（Pauli算符形式，自旋算符与Pauli算符的关系为$\vec{S} = \frac{\hbar}{2}\vec{\sigma}$）。通过斯特恩-格拉赫实验测量某个方向（例如$\vec{a}$为粒子1的测量方向）的自旋分量。&lt;/p&gt;
&lt;p&gt;测量粒子1的$\vec{\sigma_1} \cdot \vec{a}$可能得到$+1$或$-1$。由于系统反对称性（&lt;strong&gt;可补充解释&lt;/strong&gt;），此时测量粒子2的$\vec{\sigma_2} \cdot \vec{a}$结果必然与粒子1相反（$-1$或$+1$）。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;隐变量理论的数学化&#34;&gt;隐变量理论的数学化
&lt;/h3&gt;
&lt;p&gt;Bell通过以下两个假设引出隐变量原理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;局域性假设&lt;/strong&gt;：若两粒子自旋的测量在空间分离的方向进行，则一个磁铁的方向（测量粒子1的方向）不会影响另一个磁铁的测量结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;因果性假设&lt;/strong&gt;：通过测量$\sigma_1$的某个方向分量，可预测对应方向粒子2的自旋分量结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假设1指定了隐变量理论的局域性；假设2说明测量结果可被提前预测，对应隐变量$\lambda$的存在性（$\lambda$可以是单变量、一组变量或函数）。测量结果满足：&lt;/p&gt;
&lt;p&gt;$$
A(\vec{a}, \lambda) = \pm 1, \quad B(\vec{b}, \lambda) = \pm 1
$$&lt;/p&gt;
&lt;p&gt;隐变量理论的期望值为：&lt;/p&gt;
&lt;p&gt;$$
P(\vec{a}, \vec{b}) = \int d\lambda , \rho(\lambda) A(\vec{a}, \lambda) B(\vec{b}, \lambda)
$$&lt;/p&gt;
&lt;p&gt;而量子力学对自旋单态的期望值为：&lt;/p&gt;
&lt;p&gt;$$
\langle \vec{\sigma}_1 \cdot \vec{a} ; \vec{\sigma}_2 \cdot \vec{b} \rangle = -\vec{a} \cdot \vec{b}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;主要证明&#34;&gt;主要证明
&lt;/h3&gt;
&lt;p&gt;Bell不等式的推导步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;归一化条件&lt;/strong&gt;：$\int d\lambda , \rho(\lambda) = 1$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;测量结果限制&lt;/strong&gt;：$A(\vec{a}, \lambda), B(\vec{b}, \lambda) = \pm 1$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入新方向$\vec{c}$&lt;/strong&gt;，计算差值：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
P(\vec{a}, \vec{b}) - P(\vec{a}, \vec{c}) = \int d\lambda , \rho(\lambda) A(\vec{a}, \lambda) A(\vec{b}, \lambda) \left[ A(\vec{b}, \lambda) A(\vec{c}, \lambda) - 1 \right]
$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;取绝对值并利用积分性质&lt;/strong&gt;，最终得到Bell不等式：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
|P(\vec{a}, \vec{b}) - P(\vec{a}, \vec{c})| \leq 1 + P(\vec{b}, \vec{c})
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;验证实例&#34;&gt;验证实例
&lt;/h3&gt;
&lt;p&gt;取$\vec{a} = (0,0,1)$, $\vec{b} = (1,0,0)$, $\vec{c} = (1/\sqrt{2}, 0, 1/\sqrt{2})$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(\vec{a}, \vec{b}) = 0$&lt;/li&gt;
&lt;li&gt;$P(\vec{a}, \vec{c}) = -1/\sqrt{2}$&lt;/li&gt;
&lt;li&gt;$|P(\vec{a}, \vec{b}) - P(\vec{a}, \vec{c})| = 1/\sqrt{2} \approx 0.707$&lt;/li&gt;
&lt;li&gt;$1 + P(\vec{b}, \vec{c}) = 1 - 1/\sqrt{2} \approx 0.293$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然$0.707 &amp;gt; 0.293$，违反Bell不等式，说明隐变量理论无法解释量子力学预测。
$$
f(g(x)) =
\begin{cases}
0, &amp;amp; g(x) &amp;lt; 2 \
1, &amp;amp; g(x) \geq 2
\end{cases}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;结论&#34;&gt;结论
&lt;/h3&gt;
&lt;p&gt;通过证明Bell不等式在量子力学中不成立，表明非局域性是量子力学的本质特征，从而验证了量子纠缠的合理性。&lt;/p&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;
&lt;h3 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h3&gt;
&lt;h3 id=&#34;版权信息&#34;&gt;版权信息
&lt;/h3&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>毕业论文|chapter 1</title>
        <link>http://localhost:1313/2025/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87chapter-1/</link>
        <pubDate>Wed, 16 Apr 2025 16:38:41 +0800</pubDate>
        
        <guid>http://localhost:1313/2025/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87chapter-1/</guid>
        <description>&lt;h1 id=&#34;量子纠缠的早期视角&#34;&gt;量子纠缠的早期视角
&lt;/h1&gt;
&lt;h2 id=&#34;epr佯谬&#34;&gt;EPR佯谬
&lt;/h2&gt;
&lt;h3 id=&#34;一般的纠缠态&#34;&gt;一般的纠缠态
&lt;/h3&gt;
&lt;p&gt;一个最经典的纠缠态的例子就是：在计算基矢 (Computational basis) 中，如果对于一个两个量子比特的系统（包含了两个子系统 Alice 和 Bob），总状态是：&lt;/p&gt;
&lt;p&gt;$$
\ket{\psi} = \frac{1}{\sqrt{2}}(\ket{0}\ket{1} + \ket{1}\ket{0})
$$&lt;/p&gt;
&lt;p&gt;其中 $\ket{0}\ket{1}$ 表示一个复合系统，等价于 $\ket{0} \otimes \ket{1}$，左边代表系统 I 的状态，右边表示系统 II 的状态。&lt;/p&gt;
&lt;p&gt;那么对于这个系统，如果测量到 A 系统的状态是 $\ket{0}$，那么 B 系统的状态就是 $\ket{1}$；另一种情况也是一样的，这个形式与式（8）具有相似之处。&lt;/p&gt;
&lt;p&gt;所谓“纠缠”就是：对一个系统的观测结果会&lt;strong&gt;瞬间影响&lt;/strong&gt;另一个系统的状态，而且&lt;strong&gt;这种影响是非局域性的&lt;/strong&gt;（无论两个系统相距多远），这就像一种“幽灵般的超距作用”。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;epr佯谬-1&#34;&gt;EPR佯谬
&lt;/h3&gt;
&lt;p&gt;对于纠缠这个概念，最早的启发来自 Einstein 与其合作者在 1935 年发表的论文：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?&amp;rdquo;&lt;br&gt;
—— 量子力学中对物理现实的描述是完整的吗？&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;他们认为量子力学的表述是不完备的，并提出了一个纠缠系统的思考实验。他们首先定义了“物理现实中的元素”（Elements of Physical Reality）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt;&lt;br&gt;
如果我们可以在&lt;strong&gt;不干扰系统的前提下&lt;/strong&gt;，准确预测该系统中某一物理量的值，那么就存在一个物理现实的元素对应于这个物理量。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;然后他们将这个标准应用到一个复合量子系统：两个相距很远的粒子（编号为 1 和 2），其状态由如下纠缠波函数描述：&lt;/p&gt;
&lt;p&gt;$$
\psi(x_1, x_2, p_1, p_2) = \delta(x_1 - x_2 - L)\delta(p_1 + p_2)
$$&lt;/p&gt;
&lt;p&gt;其中 $\delta$ 并不是真正的 Dirac delta 函数，而是一个&lt;strong&gt;归一化的尖峰函数&lt;/strong&gt;；$L$ 是一个相对于粒子间相互作用而言非常大的距离。&lt;/p&gt;
&lt;p&gt;这个波函数的物理意义是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个粒子之间的距离几乎是 $L$；&lt;/li&gt;
&lt;li&gt;总动量几乎为 $0$；&lt;/li&gt;
&lt;li&gt;而且 $x_1 - x_2$ 和 $p_1 + p_2$ 是可同时观测的对易算符。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;💡 注：你也可以在这里插入一个 delta 函数图像来辅助理解。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对于这个状态来说，我们对单个粒子的状态（位置或动量）是一无所知的；我们只知道两个粒子之间的差值（距离、动量和）可以确定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;如果我们测量粒子 1 的位置 $x_1$，我们可以准确预测粒子 2 的位置：$x_2 = x_1 - L$。&lt;br&gt;
根据 EPR 的论点：由于两个粒子此时&lt;strong&gt;不再相互作用&lt;/strong&gt;，粒子 1 的测量&lt;strong&gt;不会干扰&lt;/strong&gt;粒子 2，因此 $x_2$ 对应着一个物理现实元素。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;同理，如果我们测量粒子 1 的动量 $p_1$，就能预测粒子 2 的动量：$p_2 = -p_1$，因此 $p_2$ 也对应一个物理现实元素。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是，根据量子力学的基本原理（不确定性原理）：&lt;/p&gt;
&lt;p&gt;$$
\Delta x \cdot \Delta p \geq \frac{\hbar}{2}
$$&lt;/p&gt;
&lt;p&gt;当粒子的位置被精确测量（$\Delta x = 0$）时，其动量的测量精度就必须变差（$\Delta p \to \infty$），所以&lt;strong&gt;不能同时确定位置与动量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;因此，EPR 论文指出：我们从测量粒子 1 就可以同时“知道”粒子 2 的位置和动量，这与量子力学的不确定性原理矛盾。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;🧠 结论：这说明量子力学对物理现实的描述是不完备的。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;EPR 因此推测：&lt;strong&gt;存在某些“隐藏变量”&lt;/strong&gt;（目前未知），使得这些物理量实际上是可以同时确定的。这就是“隐变量理论”（Hidden Variables Theory）的雏形。论文并未给出这种理论的构造，但为后来的 Bell 不等式与实验验证奠定了基础。&lt;/p&gt;
&lt;h2 id=&#34;附录&#34;&gt;附录
&lt;/h2&gt;
&lt;h3 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h3&gt;
&lt;h3 id=&#34;版权信息&#34;&gt;版权信息
&lt;/h3&gt;
&lt;p&gt;本文原载于 &lt;a class=&#34;link&#34; href=&#34;https://quantum51.top&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;quantum51.top&lt;/a&gt;，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>关于自己</title>
        <link>http://localhost:1313/%E5%85%B3%E4%BA%8E%E8%87%AA%E5%B7%B1/</link>
        <pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/%E5%85%B3%E4%BA%8E%E8%87%AA%E5%B7%B1/</guid>
        <description></description>
        </item>
        <item>
        <title>Archives</title>
        <link>http://localhost:1313/archives/</link>
        <pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/archives/</guid>
        <description></description>
        </item>
        <item>
        <title></title>
        <link>http://localhost:1313/1/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/1/</guid>
        <description>&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;###标题&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;slug&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;###描述&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;2025-04-19T09:34:28+08:00&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;lastmod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;2025-04-19T09:34:28+08:00&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;###背景图（最好是把图片放到文件夹内）&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;math&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;license&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hidden&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;draft&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;###目录&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;tags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;###标签&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;一级标题&#34;&gt;一级标题
&lt;/h1&gt;
&lt;h2 id=&#34;二级标题&#34;&gt;二级标题
&lt;/h2&gt;
&lt;h3 id=&#34;三级标题&#34;&gt;三级标题
&lt;/h3&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>http://localhost:1313/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
